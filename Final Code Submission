{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Anime Dataset\n","Part 1: Preprocessing\n","\n","This part of the notebook cleans the **Anime Recommendation** dataset to prepare it for NLP-based recommendations.\n","It follows the proposal steps: normalize text (stopword removal + lemmatization) for *synopsis/plot* and clean/standardize the *genre* field; coerce numeric fields (*episodes*, *score*); and remove duplicates/missing-critical rows."],"metadata":{"id":"63kTawGa3bgb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nh6Bmrkv11_w"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import os\n","from pathlib import Path\n","\n","# Text processing\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Ensure NLTK data is available\n","nltk.download('stopwords', quiet=True)\n","nltk.download('wordnet', quiet=True)\n","\n","DATASET_PATH = \"/content/anime_recommendation_dataset.csv\"\n","OUTPUT_CSV   = \"/mnt/data/cleaned_anime_dataset.csv\""]},{"cell_type":"code","source":["import os\n","\n","# Assuming the initial dataset is in the same Google Drive folder as other processed files.\n","# Make sure to run the cell that mounts Google Drive (nCGoeXzzJMU1) before executing this cell.\n","directory_path = '/content/drive/MyDrive/info 443'\n","DATASET_PATH = os.path.join(directory_path, 'anime_recommendation_dataset.csv')\n","\n","df = pd.read_csv(DATASET_PATH)\n","print(\"Shape:\", df.shape)\n","print(\"Columns:\", list(df.columns))\n","df.head(3)"],"metadata":{"id":"ml-guqL9184_","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1765050487790,"user_tz":300,"elapsed":116,"user":{"displayName":"Jose Contreras","userId":"07203381292022404505"}},"outputId":"71e08420-4ce1-4a2a-8d78-497230c4060b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/info 443/anime_recommendation_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3015709029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDATASET_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anime_recommendation_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/info 443/anime_recommendation_dataset.csv'"]}]},{"cell_type":"code","source":["def resolve_col(df, candidates):\n","    cols = {c.lower(): c for c in df.columns}\n","    # exact\n","    for cand in candidates:\n","        if cand.lower() in cols:\n","            return cols[cand.lower()]\n","    # contains\n","    for c in df.columns:\n","        lc = c.lower()\n","        for cand in candidates:\n","            if cand.lower() in lc:\n","                return c\n","    return None\n","\n","col_title     = resolve_col(df, [\"title\", \"name\"])\n","col_genre     = resolve_col(df, [\"genre\", \"genres\"])\n","col_synopsis  = resolve_col(df, [\"synopsis\", \"plot\", \"description\", \"summary\"])\n","col_chars     = resolve_col(df, [\"characters\", \"cast\", \"people\"])\n","col_episodes  = resolve_col(df, [\"episodes\", \"eps\", \"num_episodes\"])\n","col_score     = resolve_col(df, [\"score\", \"rating\", \"rank\", \"average_score\"])\n","\n","print({\n","    \"title\": col_title, \"genre\": col_genre, \"synopsis\": col_synopsis,\n","    \"characters\": col_chars, \"episodes\": col_episodes, \"score\": col_score\n","})"],"metadata":{"id":"ODXhsYfO3w22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["required = [col_title, col_genre, col_synopsis]\n","required = [c for c in required if c is not None]\n","if required:\n","    before = len(df)\n","    df = df.dropna(subset=required)\n","    df = df[df[required].applymap(lambda x: str(x).strip() != \"\").all(axis=1)]\n","    print(f\"Dropped {before - len(df)} rows missing required fields.\")\n","\n","if col_title:\n","    before = len(df)\n","    df = df.drop_duplicates(subset=[col_title])\n","    print(f\"Dropped {before - len(df)} duplicate titles.\")"],"metadata":{"id":"tDq12kZf34jW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if col_episodes:\n","    df[col_episodes] = pd.to_numeric(df[col_episodes], errors='coerce').fillna(0).astype(int)\n","if col_score:\n","    # keep as float; fill with column mean if missing\n","    df[col_score] = pd.to_numeric(df[col_score], errors='coerce')\n","    if df[col_score].notna().any():\n","        mean_score = df[col_score].mean()\n","        df[col_score] = df[col_score].fillna(mean_score)\n","    else:\n","        # if all NaN, set to 0\n","        df[col_score] = 0.0"],"metadata":{"id":"nXYVx00N3-Yh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_CSV  = \"/content/anime_recommendation_dataset.csv\"\n","df.to_csv(OUTPUT_CSV, index=False)\n","print(\"Saved cleaned dataset to:\", OUTPUT_CSV)\n"],"metadata":{"id":"KRlrKvmm4C91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2) Sentiment and Genre Analysis Using Vader Lexicon, MultiLabelBinarizer, and SentimentIntensityAnalyzer"],"metadata":{"id":"R6JFEwIL2GSx"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","\n","# Download VADER lexicon\n","nltk.download('vader_lexicon')\n","\n","# Load your dataset\n","df = pd.read_csv('cleaned_anime.csv')\n","# generes are set as lists in the cleaned dataset\n","df['genre'] = df['genre_tokens']\n","\n","# Encode genre labels for easier conversion/analysis\n","# mlb allows us to look at multiple genres per listing\n","mlb = MultiLabelBinarizer()\n","genre_encoded = mlb.fit_transform(df['genre'])\n","genre_df = pd.DataFrame(genre_encoded, columns=mlb.classes_)\n","\n","# Compute sentiment scores\n","sia = SentimentIntensityAnalyzer()\n","df['sentiment'] = df['clean_synopsis'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n","\n","# Combine genre features and sentiment\n","combined_features = pd.concat([genre_df, df['sentiment']], axis=1)\n","\n","print(\"Combined Feature Vectors:\")\n","print(combined_features.head())\n","# each row represents a movie w/ genre encoding (binary columns for each genre) and sentiment score (numeric value from -1 to +1)"],"metadata":{"id":"rtXMnr8p182F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalizing sentiment score to be a range of 0-1 to make calculation and display of cosine similarity matrix easier.\n","combined_features['sentiment'] = (combined_features['sentiment'] + 1) / 2\n","\n","print(\"Combined Feature Vectors after sentiment normalization:\")\n","print(combined_features.head())\n","\n","df['sentiment'] = combined_features['sentiment']\n","df['num_genres'] = genre_df.sum(axis=1)\n","# create new column for the genre score in addition to the sentiment score\n","print(\"Updated 'df' DataFrame with normalized sentiment and number of genres:\")\n","print(df[['title', 'sentiment', 'num_genres']].head())\n","\n","df['combined_score'] = df['num_genres'] + df['sentiment']\n","# make new final column that combines the scores of the genres and the sentiment\n","print(\"DataFrame with combined score:\")\n","print(df[['title', 'num_genres', 'sentiment', 'combined_score']].head())"],"metadata":{"id":"jph63udt18yD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. TF-IDF Vectorization and Cosine Similarity Matrix with Recommendation Function"],"metadata":{"id":"kHELLgxD6rp_"}},{"cell_type":"code","source":["!pip install rapidfuzz"],"metadata":{"collapsed":true,"id":"wJMHSHzMBz7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","import requests\n","from rapidfuzz import process, fuzz\n","import time\n","from IPython.display import display, HTML\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"Aija2OKp6Dlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"FP1nPBc-hUYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fetch_anime_images(df, title_col=\"title\", new_col=\"image_url\", sleep_time=1):\n","\n","    anilist_url = \"https://graphql.anilist.co\"\n","    anilist_query = \"\"\"\n","    query ($search: String) {\n","      Media(search: $search, type: ANIME) {\n","        title {\n","          romaji\n","          english\n","        }\n","        coverImage {\n","          large\n","        }\n","      }\n","    }\n","    \"\"\"\n","\n","    def get_anilist_image(title):\n","        \"\"\"Query AniList for a single title.\"\"\"\n","        try:\n","            response = requests.post(anilist_url, json={\"query\": anilist_query, \"variables\": {\"search\": title}})\n","            data = response.json()\n","            if \"data\" in data and data[\"data\"][\"Media\"] is not None:\n","                media = data[\"data\"][\"Media\"]\n","                return media[\"coverImage\"][\"large\"] or None\n","        except:\n","            return None\n","        return None\n","\n","    def get_jikan_image(title):\n","        \"\"\"Query Jikan for a single title using fuzzy matching.\"\"\"\n","        search_url = f\"https://api.jikan.moe/v4/anime?q={title}&limit=5\"\n","        try:\n","            response = requests.get(search_url)\n","            data = response.json()\n","            if \"data\" in data and len(data[\"data\"]) > 0:\n","                # Extract titles and URLs\n","                candidates = [(anime[\"title\"], anime[\"images\"][\"jpg\"][\"image_url\"]) for anime in data[\"data\"]]\n","                # Fuzzy match input title to best candidate\n","                best_match = process.extractOne(title, [c[0] for c in candidates], scorer=fuzz.ratio)\n","                if best_match:\n","                    match_index = [c[0] for c in candidates].index(best_match[0])\n","                    return candidates[match_index][1]\n","        except:\n","            return None\n","        return None\n","\n","    # Try AniList first\n","    image_urls = []\n","    for title in df[title_col]:\n","        cleaned_title = title.strip()\n","        img = get_anilist_image(cleaned_title)\n","        image_urls.append(img)\n","        time.sleep(sleep_time)  # avoid API rate limits\n","\n","    df[new_col] = image_urls\n","\n","    # Retry for na values\n","    none_indices = df[df[new_col].isna()].index\n","    if len(none_indices) > 0:\n","        print(f\"Retrying {len(none_indices)} anime with Jikan API fallback...\")\n","        for idx in none_indices:\n","            title = df.at[idx, title_col].strip()\n","            img = get_jikan_image(title)\n","            df.at[idx, new_col] = img\n","            time.sleep(sleep_time)\n","\n","    return df"],"metadata":{"id":"3K5zJ8RcCDCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nCGoeXzzJMU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory_path = '/content/drive/MyDrive/info 443'\n","\n","cos_df_path = os.path.join(directory_path, 'cos_df.csv')\n","anime_sentiment_path = os.path.join(directory_path, 'anime_with_sentiment.csv')"],"metadata":{"id":"nF9YzjZRKNKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cos_df = pd.read_csv(cos_df_path, index_col=0)\n","df = pd.read_csv(anime_sentiment_path, index_col=0)"],"metadata":{"id":"YV6AS9HZLLrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cos_df.sample(3)"],"metadata":{"id":"82pJJe5dR8Ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sample(2)"],"metadata":{"id":"8ljO8wkFheME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3 = fetch_anime_images(df, title_col=\"title\", new_col=\"cover_url\", sleep_time=1)"],"metadata":{"collapsed":true,"id":"IUjOLrMUClKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3.sample(2)"],"metadata":{"id":"GzgaPL6PNkIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3.isna().sum()"],"metadata":{"collapsed":true,"id":"wBInPoqACqF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numerical_cols = ['score', 'episodes', 'sentiment']\n","\n","scaler = MinMaxScaler()\n","\n","scaled_numerical_data = scaler.fit_transform(df3[numerical_cols])\n","\n","numerical_features_scaled_df = pd.DataFrame(scaled_numerical_data, columns=numerical_cols)"],"metadata":{"id":"w5McEiVl18ZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)\n","tv_matrix = tv.fit_transform(df3['clean_synopsis'])\n","tv_matrix = tv_matrix.toarray()\n","vocab = tv.get_feature_names_out()\n","tv_df = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"],"metadata":{"id":"7urNLEmJ18E0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = pd.concat([tv_df, numerical_features_scaled_df], axis=1)\n","cosine_sim_matrix = cosine_similarity(df2)\n","cosine_df2 = pd.DataFrame(cosine_sim_matrix)\n","cosine_df2.to_csv('/content/cos_df.csv', index=False)"],"metadata":{"id":"IkCE0Blg6MAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recommend_anime_with_covers(title):\n","\n","    lower_title = title.lower()\n","\n","    if lower_title not in df3['title'].str.lower().values:\n","        return f\"'{title}' not found in the dataset. Please provide a title from the dataset.\"\n","\n","    idx = df3[df3['title'].str.lower() == lower_title].index[0]\n","\n","    sim_scores = list(enumerate(cosine_df2.iloc[idx]))\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    # The first score is the searched item itself (score = 1.0)\n","    searched_item = sim_scores[0]\n","    recommended_items = sim_scores[1:4] # Get top 3 recommendations\n","\n","    searched_title = df3['title'].iloc[searched_item[0]]\n","    searched_cover = df3['cover_url'].iloc[searched_item[0]]\n","    searched_score = searched_item[1]\n","\n","    html_output = \"<h3 style='margin-bottom: 10px;'>Your Searched Anime:</h3>\"\n","    html_output += \"<div style='display:flex; flex-wrap: wrap; gap: 20px; margin-bottom: 30px;'>\"\n","    html_output += f\"\"\"\n","    <div style='text-align:center; width: 150px;'>\n","        <img src='{searched_cover}' alt='{searched_title}' style='width:150px; height:200px; object-fit: cover;'>\n","        <p><b>{searched_title}</b></p>\n","        <p>Similarity Score: {searched_score:.2f}</p>\n","    </div>\n","    \"\"\"\n","    html_output += \"</div>\"\n","\n","    html_output += \"<h3 style='margin-bottom: 10px;'>Recommended Anime:</h3>\"\n","    html_output += \"<div style='display:flex; flex-wrap: wrap; gap: 20px;'>\"\n","    for item in recommended_items:\n","        item_idx = item[0]\n","        item_score = item[1]\n","        rec_title = df3['title'].iloc[item_idx]\n","        rec_cover = df3['cover_url'].iloc[item_idx]\n","        html_output += f\"\"\"\n","        <div style='text-align:center; width: 150px;'>\n","            <img src='{rec_cover}' alt='{rec_title}' style='width:150px; height:200px; object-fit: cover;'>\n","            <p><b>{rec_title}</b></p>\n","            <p>Similarity Score: {item_score:.2f}</p>\n","        </div>\n","        \"\"\"\n","    html_output += \"</div>\"\n","\n","    return HTML(html_output)"],"metadata":{"id":"aXUfeNOi6L-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend_anime_with_covers('trigUN')"],"metadata":{"id":"_KHMJDgD6L7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend_anime_with_covers('Naruto')"],"metadata":{"id":"q90ViR4C6ayn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend_anime_with_covers('eYeShieLd 21')"],"metadata":{"id":"A4KxPBVKAn-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GBIT5DvSAtiv"},"execution_count":null,"outputs":[]}]}